{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考文章：[https://www.sohu.com/a/203842280_355140](https://www.sohu.com/a/203842280_355140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 情感倾向分析 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'refresh_token': '25.cbbee2edb8ed805d7668e8654e8b8dd8.315360000.1905229706.282335-19897267', 'expires_in': 2592000, 'session_key': '9mzdDASbU0oZVxNhmKUQ71j2Yfjv4w6OesukMCJ4MxoHFUb+YBx9Nxw6+6Fe3Wv0Cw/yQFJwEnrsvuTjItE0ZcPnejLIRw==', 'access_token': '24.710082c9c74f3ceefdc5c8266b22464f.2592000.1592461706.282335-19897267', 'scope': 'public nlp_simnet nlp_wordemb nlp_comtag nlp_dnnlm_cn brain_nlp_lexer brain_all_scope brain_nlp_comment_tag brain_nlp_dnnlm_cn brain_nlp_word_emb_vec brain_nlp_word_emb_sim brain_nlp_sentiment_classify brain_nlp_simnet brain_nlp_depparser brain_nlp_wordembedding brain_nlp_dnnlm_cn_legacy brain_nlp_simnet_legacy brain_nlp_comment_tag_legacy brain_nlp_lexer_custom brain_nlp_keyword brain_nlp_topic brain_nlp_ecnet brain_nlp_emotion brain_nlp_comment_tag_custom brain_nlp_news_summary brain_nlp_sentiment_classify_custom brain_nlp_address wise_adapt lebo_resource_base lightservice_public hetu_basic lightcms_map_poi kaidian_kaidian ApsMisTest_Test权限 vis-classify_flower lpq_开放 cop_helloScope ApsMis_fangdi_permission smartapp_snsapi_base iop_autocar oauth_tp_app smartapp_smart_game_openapi oauth_sessionkey smartapp_swanid_verify smartapp_opensource_openapi smartapp_opensource_recapi fake_face_detect_开放Scope vis-ocr_虚拟人物助理 idl-video_虚拟人物助理', 'session_secret': '76a6fcf60cf8ff25af47f9b098c3b0fc'}\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "import pandas as pd\n",
    "import json\n",
    "host = 'https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id=lOCpbOwkLDZEAsEvBC6AEbwq&client_secret=meXIqsu0WduxzC9fWvlSYzqTtngC604G'\n",
    "response = requests.get(host)\n",
    "if response:\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"log_id\": 6666917596511523475, \"text\": \"华为是一家伟大的公司\", \"items\": [{\"positive_prob\": 0.999888, \"confidence\": 0.999751, \"negative_prob\": 0.000111927, \"sentiment\": 2}]}\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "API_KEY = 'lOCpbOwkLDZEAsEvBC6AEbwq'\n",
    "SECRET_KEY = 'meXIqsu0WduxzC9fWvlSYzqTtngC604G'\n",
    "\n",
    "response = requests.get(\n",
    "    url='https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id={}&client_secret={}'.format(\n",
    "        API_KEY, SECRET_KEY\n",
    "    ), headers={'Content-Type': 'application/json; charset=UTF-8'})\n",
    "\n",
    "r = requests.post(\n",
    "    url='https://aip.baidubce.com/rpc/2.0/nlp/v1/sentiment_classify?charset=UTF-8&access_token={}'.format(\n",
    "        ast.literal_eval(response.text)['access_token']),\n",
    "    headers={'Content-Type': 'application/json'},\n",
    "    data='{\"text\":\"华为是一家伟大的公司\"}'.encode('utf8')\n",
    ")\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>refresh_token</th>\n",
       "      <td>25.436226d936129d82f0b4531ee8b9d8f6.315360000....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expires_in</th>\n",
       "      <td>2592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_key</th>\n",
       "      <td>9mzdAvuIT5c+OJrd59xCA1iiek/rvKofh4mPYbPHM77wDa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>access_token</th>\n",
       "      <td>24.9938ec0a5fa433dc72102af7edb4d822.2592000.15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scope</th>\n",
       "      <td>public nlp_simnet nlp_wordemb nlp_comtag nlp_d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_secret</th>\n",
       "      <td>2aaa2f7b89f980ecb4cc4b6ebb8b8f6d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                0\n",
       "refresh_token   25.436226d936129d82f0b4531ee8b9d8f6.315360000....\n",
       "expires_in                                                2592000\n",
       "session_key     9mzdAvuIT5c+OJrd59xCA1iiek/rvKofh4mPYbPHM77wDa...\n",
       "access_token    24.9938ec0a5fa433dc72102af7edb4d822.2592000.15...\n",
       "scope           public nlp_simnet nlp_wordemb nlp_comtag nlp_d...\n",
       "session_secret                   2aaa2f7b89f980ecb4cc4b6ebb8b8f6d"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.json_normalize(response.json()).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'24.710082c9c74f3ceefdc5c8266b22464f.2592000.1592461706.282335-19897267'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "access_token=response.json()[\"access_token\"]\n",
    "access_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 另一种代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'log_id': 6672163186011343539, 'text': '两人对酌山花开，一杯一杯复一杯。 我醉欲眠卿且去，明朝有意抱琴来。', 'items': [{'positive_prob': 0.950976, 'confidence': 0.891058, 'negative_prob': 0.0490238, 'sentiment': 2}]}\n",
      "{'log_id': 3653470169815939283, 'text': '山重水复疑无路，柳暗花明又一村。', 'items': [{'positive_prob': 0.901819, 'confidence': 0.781821, 'negative_prob': 0.0981807, 'sentiment': 2}]}\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "def get_accessToken():\n",
    "    AK = \"lOCpbOwkLDZEAsEvBC6AEbwq\"\n",
    "    SK = \"meXIqsu0WduxzC9fWvlSYzqTtngC604G\"\n",
    "    host = 'https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id=' + AK + '&client_secret=' + SK\n",
    "    headers = {'Content-Type': 'application/json; charset=UTF-8'}\n",
    "    response = requests.get(host, headers=headers)\n",
    "    json_result = json.loads(response.text)\n",
    "    return json_result['access_token']\n",
    "\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    access_token = get_accessToken()\n",
    "\n",
    "    url = 'https://aip.baidubce.com/rpc/2.0/nlp/v1/sentiment_classify'\n",
    "\n",
    "    params = {'access_token': access_token}\n",
    "    payload = json.dumps({'text': text})\n",
    "    headers = {'Content-Type': 'application/json; charset=UTF-8'}\n",
    "    response = requests.post(url=url,\n",
    "                             params=params,\n",
    "                             data=payload,\n",
    "                             headers=headers).json()\n",
    "    return response\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    txt1 = \"两人对酌山花开，一杯一杯复一杯。 我醉欲眠卿且去，明朝有意抱琴来。\"\n",
    "    txt2 = \"山重水复疑无路，柳暗花明又一村。\"\n",
    "    result1 = sentiment_analysis(txt1)\n",
    "    result2 = sentiment_analysis(txt2)\n",
    "    print(result1)\n",
    "    print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词向量表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词向量表示是通过训练的方法，将语言词表中的词映射成一个长度固定的向量。词表中所有的词向量构成一个向量空间，每一个词都是这个词向量空间中的一个点，利用这种方法，实现文本的可计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aip import AipNlp\n",
    "\n",
    "\n",
    "def baidu_word_emb_vec(text):\n",
    "    url = \"https://aip.baidubce.com/rpc/2.0/nlp/v2/word_emb_vec?access_token=\" + access_token\n",
    "    data = {\"word\": text}\n",
    "    encode_data = json.dumps(data).encode('UTF-8')\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    return requests.post(url, encode_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_id</th>\n",
       "      <th>word</th>\n",
       "      <th>vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2947273895539238163</td>\n",
       "      <td>小峰</td>\n",
       "      <td>[-0.705753, 0.167892, -0.314895, 0.166485, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                log_id word                                                vec\n",
       "0  2947273895539238163   小峰  [-0.705753, 0.167892, -0.314895, 0.166485, -0...."
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_word_emb_vec = baidu_word_emb_vec(\"小峰\")\n",
    "content_word_emb_vec = r_word_emb_vec.content\n",
    "content_word_emb_vec_str = str(content_word_emb_vec,encoding=\"GBK\")\n",
    "content_word_emb_vec_str_dict =json.loads(content_word_emb_vec_str)\n",
    "pd.json_normalize(content_word_emb_vec_str_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词法分析(标签、关键字 以及 词法分析)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "百度词法分析向用户提供分词、词性标注、命名实体识别三大功能。该服务能够识别出文本串中的基本词汇标注和词汇的词性，并进一步识别出命名实体，百度词法分析的算法效果大幅领先已公开的主流中文词法分析模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "from tornado.escape import json_decode\n",
    "\n",
    "\n",
    "def Tag(title, content):\n",
    "    tag = ''\n",
    "\n",
    "    APIKey = 'lOCpbOwkLDZEAsEvBC6AEbwq'\n",
    "    secret = 'meXIqsu0WduxzC9fWvlSYzqTtngC604G'\n",
    "\n",
    "    host = 'https://aip.baidubce.com/oauth/2.0/token?grant_type=client_credentials&client_id=' + APIKey + '&client_secret=' + secret\n",
    "    response = requests.get(host)\n",
    "    if response:\n",
    "        print(response.json()['access_token'])\n",
    "    kv = {\n",
    "        'accept-encoding':\n",
    "        'gzip, deflate',\n",
    "        'x-bce-date':\n",
    "        '2015-03-24T13:02:00Z',\n",
    "        'connection':\n",
    "        'keep-alive',\n",
    "        'accept':\n",
    "        '*/*',\n",
    "        'host':\n",
    "        'aip.baidubce.com',\n",
    "        'x-bce-request-id':\n",
    "        '73c4e74c-3101-4a00-bf44-fe246959c05e',\n",
    "        'content-type':\n",
    "        'application/json',\n",
    "        'authorization':\n",
    "        'bce-auth-v1/46bd9968a6194b4bbdf0341f2286ccce/2015-03-24T13:02:00Z/1800/host;x-bce-date/994014d96b0eb26578e039fa053a4f9003425da4bfedf33f4790882fb4c54903'\n",
    "    }\n",
    "    params = {\"title\": title, \"content\": content}\n",
    "    text = json.dumps(params)\n",
    "    # print(type(text))\n",
    "    url = 'https://aip.baidubce.com/rpc/2.0/nlp/v1/keyword?charset=UTF-8&access_token=' + response.json(\n",
    "    )['access_token']\n",
    "    response1 = requests.post(url, headers=kv, timeout=5, data=text)\n",
    "    if response1:\n",
    "        result = json.dumps(response1.json(),\n",
    "                            sort_keys=True,\n",
    "                            indent=4,\n",
    "                            ensure_ascii=False)\n",
    "        print(result)\n",
    "        result1 = json_decode(result)\n",
    "        for i in result1['items']:\n",
    "            tag += i['tag'] + ' '\n",
    "    return tag\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\n",
    "        Tag(\n",
    "            \"iphone手机出现“白苹果”原因及解决办法，用苹果手机的可以看下\",\n",
    "            \"如果下面的方法还是没有解决你的问题建议来我们门店看下成都市锦江区红星路三段99号银石广场24层01室。在通电的情况下掉进清水，这种情况一不需要拆机处理。尽快断电。用力甩干，但别把机器甩掉，主意要把屏幕内的水甩出来。如果屏幕残留有水滴，干后会有痕迹。^H3 放在台灯，射灯等轻微热源下让水分慢慢散去。\"\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baidu_lexer(text):\n",
    "    url = \"https://aip.baidubce.com/rpc/2.0/nlp/v1/lexer?access_token=\"+access_token\n",
    "    data = {\n",
    "        \"text\" : text   \n",
    "    }\n",
    "    encode_data=json.dumps(data).encode('UTF-8')\n",
    "    headers = {\n",
    "        'Content-Type':'application/json'\n",
    "    }\n",
    "    return requests.post(url,encode_data).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>log_id</th>\n",
       "      <td>6183939125816184915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>我爱中国</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>items</th>\n",
       "      <td>[{'loc_details': [], 'byte_offset': 0, 'uri': ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0\n",
       "log_id                                6183939125816184915\n",
       "text                                                 我爱中国\n",
       "items   [{'loc_details': [], 'byte_offset': 0, 'uri': ..."
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_lexer = baidu_lexer(\"我爱中国\")\n",
    "content_lexer_str = str(content_lexer ,encoding=\"GBK\")\n",
    "content_lexer_str_dict =json.loads(content_lexer_str)\n",
    "pd.json_normalize(content_lexer_str_dict).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词义相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用于计算两个给定词语的语义相似度，基于自然语言中的分布假设，即越是经常共同出现的词之间的相似度越高。词义相似度是自然语言处理中的重要基础技术，是专名挖掘、query 改写、词性标注等常用技术的基础之一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baidu_word_emb_sim(word_1,word_2):\n",
    "    url = \"https://aip.baidubce.com/rpc/2.0/nlp/v2/word_emb_sim?access_token=\"+access_token\n",
    "    data = {\n",
    "        \"word_1\":word_1,\n",
    "        \"word_2\":word_2\n",
    "    }\n",
    "    encode_data=json.dumps(data).encode('UTF-8')\n",
    "    headers = {\n",
    "        'Content-Type':'application/json'\n",
    "    }\n",
    "    return requests.post(url,encode_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>log_id</th>\n",
       "      <td>7675891744259668691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>0.378168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words.word_2</th>\n",
       "      <td>广东</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words.word_1</th>\n",
       "      <td>广西</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0\n",
       "log_id        7675891744259668691\n",
       "score                    0.378168\n",
       "words.word_2                   广东\n",
       "words.word_1                   广西"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_word_emb_sim = baidu_word_emb_sim(\"广西\",\"广东\")\n",
    "content_word_emb_sim = r_word_emb_sim.content\n",
    "content_word_emb_sim_str = str(content_word_emb_sim ,encoding=\"GBK\")\n",
    "content_word_emb_sim_str_dict =json.loads(content_word_emb_sim_str)\n",
    "pd.json_normalize(content_word_emb_sim_str_dict).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中文DNN语言模型接口用于输出切词结果并给出每个词在句子中的概率值,判断一句话是否符合语言表达习惯。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baidu_dnnlm_cn(text):\n",
    "    url = \"https://aip.baidubce.com/rpc/2.0/nlp/v2/dnnlm_cn?access_token=\"+access_token\n",
    "    data = {\n",
    "        \"text\" : text   \n",
    "    }\n",
    "    encode_data=json.dumps(data).encode('UTF-8')\n",
    "    headers = {\n",
    "        'Content-Type':'application/json'\n",
    "    }\n",
    "    return requests.post(url,headers=headers,data=encode_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_dnnlm_cn = baidu_dnnlm_cn(\"5年高考3年模拟\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>log_id</th>\n",
       "      <td>4820569499341071923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>5年高考3年模拟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>items</th>\n",
       "      <td>[{'word': '5', 'prob': 0.000865133}, {'word': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppl</th>\n",
       "      <td>63.2819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0\n",
       "log_id                                4820569499341071923\n",
       "text                                             5年高考3年模拟\n",
       "items   [{'word': '5', 'prob': 0.000865133}, {'word': ...\n",
       "ppl                                               63.2819"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_dnnlm_cn = r_dnnlm_cn.content\n",
    "content_dnnlm_cn_str = str(content_dnnlm_cn,encoding=\"GBK\")\n",
    "content_dnnlm_cn_str_dict =json.loads(content_dnnlm_cn_str)\n",
    "pd.json_normalize(content_dnnlm_cn_str_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': '5', 'prob': 0.000865133},\n",
       " {'word': '年', 'prob': 0.0319413},\n",
       " {'word': '高考', 'prob': 0.00549063},\n",
       " {'word': '3', 'prob': 0.0169704},\n",
       " {'word': '年', 'prob': 0.395461},\n",
       " {'word': '模拟', 'prob': 0.0152922}]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_dnnlm_cn_str_dict[\"items\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 依存句法分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baidu_yicunjufa_cn(text,mode):\n",
    "    url = \"https://aip.baidubce.com/rpc/2.0/nlp/v1/depparser?access_token=\"+access_token\n",
    "    data = {\n",
    "        \"text\" : text,\n",
    "        \"mode\" : 1\n",
    "    }\n",
    "    encode_data=json.dumps(data).encode('UTF-8')\n",
    "    headers = {\n",
    "        'Content-Type':'application/json'\n",
    "    }\n",
    "    return requests.post(url,headers=headers,data=encode_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>log_id</th>\n",
       "      <td>210745790726934899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>手机缝隙灰尘怎么清除</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>items</th>\n",
       "      <td>[{'postag': 'n', 'head': 2, 'word': '手机', 'id'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0\n",
       "log_id                                 210745790726934899\n",
       "text                                           手机缝隙灰尘怎么清除\n",
       "items   [{'postag': 'n', 'head': 2, 'word': '手机', 'id'..."
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_yicunjufa_cn = baidu_yicunjufa_cn(\"手机缝隙灰尘怎么清除\",\"1\")\n",
    "content_yicunjufa_cn = r_yicunjufa_cn.content\n",
    "content_yicunjufa_cn_str = str(content_yicunjufa_cn, encoding=\"GBK\")\n",
    "content_yicunjufa_cn_str_dict = json.loads(content_yicunjufa_cn_str)\n",
    "pd.json_normalize(content_yicunjufa_cn_str_dict).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 短文本相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baidu_text_emb_sim(text_1,text_2):\n",
    "    url = \"https://aip.baidubce.com/rpc/2.0/nlp/v2/simnet?access_token=\"+access_token\n",
    "    data = {\n",
    "        \"text_1\":text_1,\n",
    "        \"text_2\":text_2\n",
    "    }\n",
    "    encode_data=json.dumps(data).encode('UTF-8')\n",
    "    headers = {\n",
    "        'Content-Type':'application/json'\n",
    "    }\n",
    "    return requests.post(url,encode_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>log_id</th>\n",
       "      <td>1857610533435825491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>0.330024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texts.text_2</th>\n",
       "      <td>万事通自考网</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texts.text_1</th>\n",
       "      <td>浙富股份</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0\n",
       "log_id        1857610533435825491\n",
       "score                    0.330024\n",
       "texts.text_2               万事通自考网\n",
       "texts.text_1                 浙富股份"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_text_emb_sim = baidu_text_emb_sim(\"浙富股份\",\"万事通自考网\")\n",
    "content_text_emb_sim = r_text_emb_sim.content\n",
    "content_text_emb_sim_str = str(content_text_emb_sim ,encoding=\"GBK\")\n",
    "content_text_emb_sim_str_dict =json.loads(content_text_emb_sim_str)\n",
    "pd.json_normalize(content_text_emb_sim_str_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
